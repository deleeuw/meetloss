

```{r ca, echo=FALSE}
source("rcode/gifiEngine.R")
source("rcode/gifiUtilities.R")
source("rcode/gifiWrappers.R")
source("rcode/gifiStructures.R")
source("rcode/matrix.R")
source("rcode/coneRegression.R")
source("rcode/splineBasis.R")
```

# Correspondence Analysis and corals()

## Introduction

Ordinary correspondence analysis (CA) is the special case of MCA in which there are only two variables, and both variables have the maximum number of copies. Consequently the `homals()` wrapper can be used to compute a CA. Because input and output can be organized a bit differently for CA we have written the separate wrapper `corals()`.

Note that `corals()` is not really intended for routine CA computation. There are many packages in R which do that job much more efficiently. We mention, for example, `anacor` (@deleeuw_mair_A_09b) and `ca` (@nenadic_greenacre_07).
However, `corals()` can be used for a number of cases which the usual CA packages do not cover.

In `corals()`, as in the other packages, the default input is a single non-negative matrix $F$. Although any non-negative matrix will do, the most common, and the most natural, input is an $r\times c$ *cross table* with bivariate frequencies. Suppose the frequencies add up to the total number of observations $n$. Then `gifiEngine()`, which is called by `corals()`, requires input in the form of an $n\times 2$ matrix. Thus a $2\times 2$ table with 1000 observations becomes a $1000\times 2$ matrix. The utility `preCorals()` makes the conversion, but of course the representation is embarrassingly inefficient, both in terms of memory and in terms of computation. After the computations are done, the utility `postCorals()` restores transformations and scores to the appropriate row and column dimensions.

Here are the arguments and their defaults.

```{r corals_args, echo = FALSE}
args(corals)
```
If dtype is `FALSE`, then data is a matrix of dimension $n\times 2$, with $n$ the number of observations. This takes us back to the input format of `homals()` with two variables. If xknots and yknots are kept to their default `NULL`
then they are replaced in `corals()` by the quartiles of the two variables.

The redeeming feature of `corals()` is that, unlike the other classical CA packages, it can handle numerical variables, it can incorporate higher order splines, it can impose monotonicity restrictions, and it can deal with missing data in one of both of the variables. If there are supplementary variables then it makes more sense to use `homals()`.

## Equations

The usual stationary equations for CA, using the category quantifications $Y_1$ and $Y_2$ are
\begin{align*}
C_{12}Y_2&=D_1Y_1\Lambda,\\
C_{21}Y_1&=D_2Y_2\Lambda,
\end{align*}
with normalization $Y_1'D_1Y_1=I$ and $Y_2'D_2Y_2=I$. 

In the output of `gifiEngine()` the category quantifications $\tilde Y_1$ and $\tilde Y_2$ and the object scores $X$ satisfy
\begin{align*}
G_1\tilde Y_1+G_2\tilde Y_2&=2X\tilde\Lambda,\\
D_1^{-1}G_1'X&=\tilde Y_1,\\
D_2^{-1}G_2'X&=\tilde Y_2,
\end{align*}
with normalization $X'X=I$. It follows that
\begin{align*}
C_{12}\tilde Y_2&=D_1\tilde Y_1(2\tilde\Lambda-I),\\
C_{21}\tilde Y_1&=D_2\tilde Y_2(2\tilde\Lambda-I),
\end{align*}
and thus for the discrimination matrices $\tilde Y_1'D_1\tilde Y_1=\tilde Y_2'D_2\tilde Y_2=X'P_1X=X'P_2X=\tilde\Lambda$. The two sets of quantities from CA and `corals()` are related by $\Lambda=2\tilde\Lambda-I$, $Y_1=\tilde Y_1\tilde\Lambda^{-\frac12}$ and $Y_2=\tilde Y_2\tilde\Lambda^{-\frac12}$. 

In classical CA there is no direct equivalent of the object scores $X$. Also we typically do not use the decomposition $H_jA_j=G_jZ_jA_j=G_jY_j$, with $H_j'H_j=Z_j'D_jZ_j=I$. From `corals()` we get the loadings $H_j'X$, the correlations between the object scores and transformed copies, which for singleton blocks are always equal to the weights $A_j$. But since the decomposition $Y_j=Z_jA_j$ is not unique these are of limited use. The correlations
between $X$ and the $G_j\tilde Y_j$ are more interesting. Since $X'G_j\tilde Y_j=\tilde\Lambda$, we see these correlations are equal to $\tilde\Lambda^\frac12$.


## Examples

### Glass

We start with a classical CA example that was also used by @gifi_B_90 (p 277-280) and by @deleeuw_mair_A_09b. Data are from @glass_54. Occupational status of the fathers is crossed with occupational status of the son, for 3497 British families. The row (father) and column (son) categories are

* PROF professional and high administrative
* EXEC managerial and executive
* HSUP higher supervisory
* LSUP lower supervisory
* SKIL skilled manual and routine nonmanual
* MEMI semi-skilled manual
* UNSK unskilled manual

```{r read_glass_data}
data (glass, package = "anacor")
names <- c("PROF","EXEC","HSUP","LSUP","SKIL","MEMI","UNSK")
glass <- as.matrix (glass)
```
We apply apply `corals()` with the default options. Thus we only compute two dimensions and use crisp indicators.
```{r glass_degree_2, cache = TRUE}
h <- corals(glass)
```

Minimum loss is `r h$f`, attained after `r h$ntel` iterations. The two discrimination matrices are both equal to

```{r glass_lambda, echo = FALSE}
mprint(h$lambda)
```
which means the corresponding canonical correlations are `r 2*diag(h$lambda)-1`. The maximum correlation
between SES of father and son is `r 2*diag(h$lambda)[1]-1`.

The category quantifications for fathers are

```{r glass_quants_fathers, echo = FALSE}
mprint(h$xquantifications, d = 4, w = 8)
```
and for sons
```{r glass_quants_sons, echo = FALSE}
mprint(h$yquantifications, d = 4, w = 8)
```
We did not require the first dimension to be increasing, it just came out that way. We plot category quantifications in figure.
<hr>
```{r glass-quantifications, fig_align = "center", echo = FALSE}
par(mfrow=c(1,2), pty = "s")
plot(h$xquantifications, xlab = "dimension 1", ylab = "dimension 2", type = "n", main = "Fathers")
text(h$xquantifications, names, col = "RED", cex = .5)
plot(h$yquantifications, xlab = "dimension 1", ylab = "dimension 2", type = "n", main = "Sons")
text(h$yquantifications, names, col = "RED", cex = .5)
```
<center>
</center>
<hr>

The 3497 objectscores can take only 49 different values, of which only 47 actually occur in the data. They are plotted in figure . Point labels are first letters of the two corresponding SES categories, first letter for the fathers, second letter for the sons.
<hr>
```{r galo_objectscore_plot, echo = FALSE, fig.align = "center", fig.width = 6, fig.asp = 1}
a <- array (0, c(dim(glass), 2))
for (i in 1:nrow(glass)) {
  for (j in 1:nrow(glass)) {
  a[i,j,]<-(h$xquantifications[i,]+h$yquantifications[j,])/ diag(h$lambda)
  }
}
plot(0, xlim=c(min(a[,,1]),max(a[,,1])), ylim=c(min(a[,,2]),max(a[,,2])), xlab = "dimension 1", ylab = "dimension 2", type = "n")
for (i in 1:nrow(glass)) {
  r <- substr(names[i],1,1)
  for (j in 1:nrow(glass)) {
  s <- substr(names[j],1,1)
  rs <- paste (r, s, sep = "")
  text (a[i,j,1], a[i,j,2], rs, col = "RED", cex = .75)
  }
}
```
<center>

</center>
<hr>
Next, we look at *regression plots*, made with the utility `regressionPlotter()`. One-dimensional category quantifications for rows and columns are used to lCAte row and column categories on the horizontal and vertical axes. Frequencies from the table are used to label the intersections of the corresponding vertical and horizontal lines. We then compute the regression lines, using row and column averages of the category quantifications, for these transformed variables. In the first plot we see what happens if we use equally spaced scores for the 
categories of both fathers and sons. Regressions are not quite linear. Then we use the first dimension of the
CA quantifications, which linearizes the regressions. And in the third plot we use the second dimension, which again linearizes the regressions, but permutes the rows and colums of the table.

<hr>
```{r glass_regression_plot, fig.align="center", fig.width = 6, fig.height = 18, echo = FALSE}
par (mfrow = c(3,1))
regressionPlotter(glass,1:nrow(glass),1:ncol(glass), xname= "Sons", yname = "Fathers", main = "Integer Quantifications")
regressionPlotter(glass,h$xquantifications[,1],h$yquantifications[,1], xname= "Sons", yname = "Fathers", main = "First Corals Dimension")
regressionPlotter(glass,h$xquantifications[,2],h$yquantifications[,2], xname= "Sons", yname = "Fathers", main = "Second Corals Dimension")
```
<center>
</center>
<hr>

### Galton

To illustrate some of the additional `corals()` options we use the classical father-child RFF height data of @galton_89. It has mid-parent height in the rows and mid-adult-child height in the columns.

```{r galton_data}
data (galton, package = "anacor")
galton <- as.matrix (galton)
galton <- galton[nrow (galton):1, ]
galton
```
```{r galton_degree_0, echo = FALSE, cache = TRUE}
h<-corals(galton, ndim=1)
```
The regression plots from a one-dimensional `corals()`, with default options, in the familiar before and after
format, are in figure.
<hr>
```{r galton_regression_plots, fig.align = "center", fig.width = 6, fig.height = 14, echo = FALSE}
par(mfrow=c(2,1))
regressionPlotter(galton,1:nrow(galton),1:ncol(galton), xname= "Children", yname = "Parents", main = "Integer Quantifications")
regressionPlotter(galton,h$xtransform[, 1],h$ytransform[, 1], xname= "Children", yname = "Parents", main = "Corals Quantifications")
```
<center>
</center>
<hr>

We see some deviations from monotonicity and the ends of the scale, where some columns of the table are interchanged. This is presumably because of the small number of observations in the extreme categories. We repeat the analysis with
ordinal transformations of degree 2 (i.e. piecewise quadratics, differentiable at the knots, and monotone at the data points) and equally spaced knots.
```{r galton_degree_2, cache = TRUE}
galton_knots = c(2, 4, 6, 8, 10)
h <- corals(
  galton,
  ndim = 1,
  xord = TRUE,
  yord = TRUE,
  xdeg = 2,
  ydeg = 2,
  xknots = galton_knots,
  yknots = galton_knots
)
```
The transformations of the variables are in figure. They show some clear deviations from linearity.
```{r galton_transformation_plots, fig.align = "center", fig.width = 6, fig.height = 14, echo = FALSE}
par(mfrow=c(2,1))
plot(1:nrow(galton), h$xtransform[,1], xlab = "Row Value", ylab = "Transform", type = "l", lwd = 2, col = "RED")
for (i in 1:5) abline(v = galton_knots[i])
plot(1:ncol(galton), h$ytransform[,1], xlab = "Column Value", ylab = "Transform", type = "l", lwd = 2, col = "RED")
for (i in 1:5) abline(v = galton_knots[i])
```
<center>

</center>
<hr>