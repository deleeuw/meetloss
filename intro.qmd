# Introduction

One way of looking at  _Multivariate Analysis with Optimal Scaling_, or *MVAOS*, is as an extension of classical linear multivariate analysis to variables that are binary, ordered, or even unordered categorical. In R terminology, classical MVA techniques can thus be applied if some or all of the variables in the dataframe are *factors*. Categorical variables are *quantified* and numerical variables are *transformed* to optimize the linear or bilinear least squares fit.

Least squares and eigenvalue methods for quantifying multivariate qualitative data were first introduced by @guttman_41, although there were some bivariate predecessors in the work of Pearson, Fisher, Maung, and Hirschfeld (see @deleeuw_A_83b or @gower_90 for a historical overview). In this earlier work the emphasis was often on optimizing quadratic forms, or ratios of quadratic forms, and not so much on least squares, distance geometry, and graphical representations such as _biplots_ (@gower_hand_96, @gower_leroux_gardnerlubbe_15, @gower_leroux_gardnerlubbe_16). They were taken up by, among others, @deleeuw_R_68e, by Benz√©cri and his students in France (see @cordier_65), and by Hayashi and his students in Japan (see @tanaka_79). Early applications can be found in ecology, following an influential paper by @hill_74. With increasing emphasis on software the role of graphical representations has increased and continues to increase.

In @deleeuw_B_74 a first attempt was made to unify most classical descriptive multivariate techniques using a single least squares loss function and a corresponding _alternating least squares (ALS)_ optimization method. His work then bifurcated to the *ALSOS project*, with Young and Takane at the University of North Carolina Chapell Hill, and the *Gifi project*, at the Department of Data Theory of Leiden University.

The ALSOS project was started in 1973-1974, when De Leeuw was visiting Bell Telephone Labs in Murray Hill. ALSOS stands for Alternating Least Squares with Optimal Scaling. The ALS part of the name was provided by @deleeuw_R_68d and the OS part by @bock_60. At early meetings of the Psychometric Society some members were offended by our use of "Optimal Scaling", because they took it to imply that their methods of scaling were supposedly inferior to ours. But the "optimal" merely refers to optimality in the context of a specific least squares loss function.

Young, De Leeuw, and Takane applied the basic ALS and OS methodology to conjoint analysis, regression, principal component analysis, multidimensional scaling, and factor analysis, producing computer programs (and SAS modules) for each of the techniques. An overview of the project, basically at the end of its lifetime, is in @young_deleeuw_takane_C_80 and @young_81.

The ALSOS project was clearly inspired by the path-breaking work of @kruskal_64a and @kruskal_64b, who designed a general way to turn metric multivariate analysis techniques into non-metric ones. In fact, Kruskal applied the basic methodology developed for multidimensional scaling to linear models in @kruskal_65, and to principal component analysis in @kruskal_shepard_74 (which was actually written around 1965 as well). In parallel developments closely related nonmetric methods were developed by @roskam_68 and by Guttman and Lingoes (see @lingoes_73). 

The Gifi project took its inspiration from Kruskal, but perhaps even more from @guttman_41 (and to a lesser extent from the optimal scaling work of Fisher, see @gower_90). Guttman's quantification method, which later became known as _multiple correspondence analysis_, was merged with linear and nonlinear principal component analysis in the HOMALS/PRINCALS techniques and programs (@deleeuw_vanrijckevorsel_C_80). The MVAOS loss function that was chosen ultimately, for example in the work of @vanderburg_deleeuw_verdegaal_A_88, had been used earlier by @carroll_68 in multi-set canonical correlation analysis of numerical variables.

A project similar to ALSOS/Gifi was ACE, short for *Alternating Conditional Expectations*. The ACE method for regression was introduced by @breiman_friedman_85 and the ACE method for principal component analysis by
@koyak_87. Both techniques use the same ALS block relaxation methods, but instead of projecting on a cone or subspace of possible transformation, they apply a smoother (typically Friedman's supersmoother) to find the
optimal transformation. This implies that the method is intended primarily for continuous variables, and that the convergence properties of the ACE algorithm are more complicated than those of a proper ALS algorithms.

An even more closely related project, by Winsberg and Ramsay, uses the cone of I-splines (integrated B-splines)
to define the optimal transformations. The technique for linear models is in @winsberg_ramsay_80 and the one for 
principal component analysis in @winsberg_ramsay_83. Again, the emphasis on monotonic splines indicates that
continuous variables play a larger role than in the ALSOS or Gifi system.

So generally there have been a number of projects over the last 50 years that differ in detail, but apply basically the same methodology (alternating least squares and optimal scaling) to generalize classical MVA techniques. Some of them emphasize transformation of continuous variables, some emphasize quantification of discrete variables. Some emphasize monotonicity, some smoothness. Usually these projects include techniques for regression and principal component analysis, but in the case of Gifi the various forms of correspondence analysis and canonical analysis are also included.

## Beyond Gifi

The techniques discussed in @gifi_B_90, and implemented in the corresponding computer programs, use a particular least squares loss function and minimize it by alternating least squares algorithms. All techniques use what Gifi calls *meet loss*, which is basically the loss function proposed by @carroll_68 for multiset canonical correlation analysis. Carroll's work was extended in Gifi by using optimal scaling to transform or quantify variables coded with indicators, and to use constraints on the parameters to adapt the basic technique, often called *homogeneity analysis*, to different classical MVA techniques.

There have been various extensions of the classical Gifi repertoire by adding
techniques that do not readily fit into meet loss. Examples are  path analysis (@coolen_deleeuw_R_87), linear dynamic systems (@bijleveld_deleeuw_A_91), and factor analysis (@deleeuw_C_04a). But adding these techniques does not really add up to a new framework.

Somewhat more importantly, @deleeuw_vanrijckevorsel_C_88 discuss various ways to generalize meet loss by using *fuzzy coding*. Transformations are no longer step functions, and coding can be done with fuzzy indicators, such as B-spline bases.
This makes it easier to deal with variables that have many ordered categories. Although this is a substantial generalization the basic framework remains the same.

One of the outgrowths of the Gifi project was the *aspect* approach, first discussed systematically by @deleeuw_C_88b, and implemented in the R package
`aspect` by @mair_deleeuw_A_10. In its original formulation it uses *majorization* to optimize functions defined on the space of correlation matrices, where the correlations are computed over transformed variables, coded by indicators.  Thus we optimize aspects of the correlation matrix over transformations of the variables. The `aspect` software was recently updated to allow for B-spline transformations (@deleeuw_E_15b). Many different aspects were implemented, based on eigenvalues, determinants, multiple correlations, and sums of powers of correlation coefficients. Unformately, aspects defined in terms of canonical correlations, or generalized canonical correlations, were not covered. Thus the range of techniques covered by the `aspect` approach has multiple regression and principal component analysis in common with the range of the Gifi system, but is otherwise disjoint from it.

In @deleeuw_C_04a a particular correlation aspect was singled out that could bridge the gap between the aspect approach and the Gifi approach, provided  *orthoblocks* of transformations were introduced. This is combined with the notion of *copies*, introduced in @deleeuw_C_84c, to design a new class of techniques that encompasses all of Gifi and that brings generalized canonical correlation analysis in the aspect framework. Thus correlation aspects, and the majorization algorithms to optimize them, are now a true generalization of the Gifi system. 


*** This is the system we discuss in this book.
