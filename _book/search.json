[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gifi Analysis of Multivariate Data",
    "section": "",
    "text": "Preface\nIn 1980 members of the Department of Data Theory at the University of Leiden taught a post-doctoral course in Nonlinear Multivariate Analysis. The course content was sort-of-published, in Dutch, as Gifi (1980). The course was repeated in 1981, and this time the sort-of-published version (Gifi (1981)) was in English.\nThe preface gives some details about the author.\n\nThe text is the joint product of the members of the Department of Data Theory of the Faculty of Social Sciences, University of Leiden. ‘Albert Gifi’ is their ‘nom de plume’. The portrait, however, of Albert Gifi shown here, is that of the real Albert Gifi to whose memory this book is dedicated, as a far too late recompense for his loyalty and devotion, during so any years, to the Cause he served.\n\nRoughly ten years later a revised version of these course notes came out as an actual book in the Wiley Series in Probabilty and Mathematical Statistics (Gifi (1990)). This despite the fact that the contents of the book had very little to do with either probability or mathematical statistics. The book is organized around a series of computer programs for correspondence analysis, principal component analysis, and canonical analysis. The programs, written in FORTRAN, are called HOMALS, PRINCALS, PRIMALS, CRIMINALS, CANALS, OVERALS because they combine classical linear multivariate analysis with optimal transformation of the variables, using alternating least squares (or ALS). It serves, to some extent, as a manual for the programs, but it also discusses the properties of the techniques implemented in the programs, and it presents many detailed applications of these techniques.\nReviewers generally had some difficulties separating the wheat from the chaff.\n\nAs the spirit of Albert Gifi has faded away, so has his whimsical approach to publishing, and his latest book is an idiosyncratic account of multivariate methods developed by the Leiden group during the 1970s. The names of their computer programs are distinguished by the ending ~ALS, thus we have OVERALS, PRINCALS, HOMALS, CANALS, MORALS, MANOVALS, CRIMINALS, PARTALS and PATHALS. Perhaps if you have a warped mind like this reviewer, you will turn rapidly to CRIMINALS. What can it be ? Surely it must give some illicit view of the truth about the world, a vision of the underworld of multivariate analysis ? Alas no ! It turns out only to be a synonym of Canonical Variate Analysis, sometimes known as Multiple Discriminant Analysis. Likewise HOMALS turns out to be Reciprocal Averaging, otherwise known as Correspondence Analysis. (Hill (1990))\n\nThis ambiguity and confusion are not too surprising. The Gifi book was a summary of the work of a large number of people, over a period of almost 20 years. Nevertheless, and perhaps because of this, it is somewhat of a camel, which we define for our purposes as a horse designed by a committee. Different chapters had different authors, and the common ideas behind the various techniques were not always clearly explained.\n\nIn Gifi’s MVA the criterion called “meet” loss plays a central role. Although the adoption of this criterion is one of the most important contributions of Gifi, the book would have been much more readable if this criterion had been introduced right at the outset and was followed throughout the rest of the book. (Takane (1992))\n\nNevertheless there is much original material in Gifi (1990), and the book has early applications of alternating least squares, majorization, coordinate descent, the delta method, and the bootstrap. And it emphasizes throughout the idea that statistics is about techniques, not about models. But, yes, the organization leaves much to be desired. An on demand printing of the first and only edition is now available on Amazon for $ 492 – although of course used versions go for much less.\nThe book was published by a prestiguous publisher in a prestiguous series, but it is fair to say it never really caught on. It is not hard to understand why. The content, and the style, are unfamiliar to statisticians and mathematicians. There is no inference, no probability, and very little rigor. The content is in multivariate data analysis, which would be most at home these days, if anywhere, in a computer science department. The Gifi group did not have the resources of, say, Benzécri in France or Hayashi in Japan. The members were mostly active in psychometrics, a small and insular field, and they were from The Netherlands, a small country prone to overestimate its importance (Marvell (1653)). They also did not have the evangelical zeal necessary for creating and sustaining a large impact.\nThere have been some other major publication events in the Gifi saga. Around the same time as the Wiley book there was the publication of SPSS (1989). Starting in the late seventies the Gifi FORTRAN programs had been embedded in the SPSS system. The SPSS Categories manual was updated many times, in fact every time SPSS or IBM SPSS had a new release. Over the years other programs produced by the Department of Data Theory were added. A recent version is, for example, Meulman and Heiser (2012), corresponding to IBM SPSS 21. It acknowledges the contributions of some of the members of the Gifi team – but in IBM (2015), the version for IBM SPSS 23, these acknowledgements and the names of the authors have disappeared. Sic transit gloria mundi.\nMichailidis and De Leeuw (1998) made an attempt to make the Gifi material somewhat more accessible by publishing a review article in a widely read mainstream statistical journal. Another such attempt is De Leeuw and Mair (2009), in which the homals package for R is introduced. The homals package is basically a single monolithic R function that can do everything the Gifi programs can do, and then some. In both cases, however, the problem remained that the techniques, and the software, were too convoluted and too different from what both statisticians and users were accustomed to.\nVan der Heijden and Van Buuren (1916) give an excellent, though somewhat wistful, historic overview of the Gifi project. It is too early for eulogies, however, and we refuse to give up. This book is yet another reorganization of the Gifi material, with many extensions. We take Yoshio Takane’s advice seriously, and we organize both the theory and the algorithms around what is called “meet-loss” in Gifi. In our software we separate the basic computational engine from its various applications that define the techniques of Multivariate Analysis with Optimal Scaling (MVAOS). Hiding the core makes it possible to make the programs behave in much the same way as traditional MVA programs. The software is written in R ((r_core_team_16?)), with some parts of the computational engine written in C.\nThe book itself is written in Rmarkdown, using bookdown (Xie (2016)) and knitr (Xie (2015)) to embed the computations and graphics, and to produce html and pdf versions that are completely reproducible. The book and all the files that go with it are in the public domain.\nWe would like to acknowledge those who have made substantial contributions to the Gifi project (and its immediate ancestors and offspring) over the years. Some of them are lost in the mists of time, some of them are no longer on this earth. They are, in alphabetical order, Bert Bettonvil, Jason Bond, Catrien Bijleveld, Frank Busing, Jacques Commandeur, Henny Coolen, Steef de Bie, Jan de Leeuw, John Gower, Patrick Groenen, Chris Haveman, Willem Heiser, Abby Israels, Judy Knip, Jan Koster, Pieter Kroonenberg, Patrick Mair, Adriaan Meester, Jacqueline Meulman, George Michailidis, Peter Neufeglise, Dré Nierop, Ineke Stoop, Yoshio Takane, Stef van Buuren, John van de Geer, Gerda van den Berg, Eeke van der Burg, Peter van der Heijden, Anita van der Kooij, Ivo van der Lans, Rien van der Leeden, Jan van Rijckevorsel, Renée Verdegaal, Peter Verboon, Susañña Verdel, and Forrest Young.\n\n\n\n\nDe Leeuw, J., and P. Mair. 2009. “Homogeneity Analysis in R: the Package homals.” Journal of Statistical Software 31 (4): 1–21. https://www.jstatsoft.org/v31/i04/.\n\n\nGifi, A. 1980. Niet-Lineaire Multivariate Analyse [Nonlinear Multivariate Analysis]. Leiden, The Netherlands: Department of Data Theory FSW/RUL.\n\n\n———. 1981. Nonlinear Multivariate Analysis. Leiden, The Netherlands: Department of Data Theory FSW/RUL.\n\n\n———. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nHill, M. O. 1990. “Review of A. Gifi, Multivariate Analysis.” Journal of Ecology 78 (4): 1148–49.\n\n\nIBM. 2015. IBM SPSS Categories 23. IBM Corporation.\n\n\nMarvell, A. 1653. “The Character of Holland.”\n\n\nMeulman, J. J., and W. J. Heiser. 2012. IBM SPSS Categories 21. IBM Corporation.\n\n\nMichailidis, G., and J. De Leeuw. 1998. “The Gifi System for Descriptive Multivariate Analysis.” Statistical Science 13: 307–36.\n\n\nSPSS. 1989. SPSS Categories. SPSS Inc.\n\n\nTakane, Y. 1992. “Review of Albert Gifi, Nonlinear Multivariate Analysis.” Journal of the American Statistical Association 87: 587–88.\n\n\nVan der Heijden, P. G. M., and S. Van Buuren. 1916. “Looking Back at the Gifi System of Nonlinear Multivariate Analysis.” Journal of Statistical Software 73 (4).\n\n\nXie, Y. 2015. Dynamic Documents with R and knitr. Second Edition. CRC Press.\n\n\n———. 2016. Bookdown: Authoring Books with r Markdown.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Beyond Gifi\nOne way of looking at Multivariate Analysis with Optimal Scaling, or MVAOS, is as an extension of classical linear multivariate analysis to variables that are binary, ordered, or even unordered categorical. In R terminology, classical MVA techniques can thus be applied if some or all of the variables in the dataframe are factors. Categorical variables are quantified and numerical variables are transformed to optimize the linear or bilinear least squares fit.\nLeast squares and eigenvalue methods for quantifying multivariate qualitative data were first introduced by Guttman (1941), although there were some bivariate predecessors in the work of Pearson, Fisher, Maung, and Hirschfeld (see De Leeuw (1983) or Gower (1990) for a historical overview). In this earlier work the emphasis was often on optimizing quadratic forms, or ratios of quadratic forms, and not so much on least squares, distance geometry, and graphical representations such as biplots (Gower and Hand (1996), Gower, Le Roux, and Gardner-Lubbe (2015), Gower, Le Roux, and Gardner-Lubbe (2016)). They were taken up by, among others, De Leeuw (1968a), by Benzécri and his students in France (see Cordier (1965)), and by Hayashi and his students in Japan (see Tanaka (1979)). Early applications can be found in ecology, following an influential paper by Hill (1974). With increasing emphasis on software the role of graphical representations has increased and continues to increase.\nIn De Leeuw (1974) a first attempt was made to unify most classical descriptive multivariate techniques using a single least squares loss function and a corresponding alternating least squares (ALS) optimization method. His work then bifurcated to the ALSOS project, with Young and Takane at the University of North Carolina Chapell Hill, and the Gifi project, at the Department of Data Theory of Leiden University.\nThe ALSOS project was started in 1973-1974, when De Leeuw was visiting Bell Telephone Labs in Murray Hill. ALSOS stands for Alternating Least Squares with Optimal Scaling. The ALS part of the name was provided by De Leeuw (1968b) and the OS part by Bock (1960). At early meetings of the Psychometric Society some members were offended by our use of “Optimal Scaling”, because they took it to imply that their methods of scaling were supposedly inferior to ours. But the “optimal” merely refers to optimality in the context of a specific least squares loss function.\nYoung, De Leeuw, and Takane applied the basic ALS and OS methodology to conjoint analysis, regression, principal component analysis, multidimensional scaling, and factor analysis, producing computer programs (and SAS modules) for each of the techniques. An overview of the project, basically at the end of its lifetime, is in Young, De Leeuw, and Takane (1980) and Young (1981).\nThe ALSOS project was clearly inspired by the path-breaking work of Kruskal (1964a) and Kruskal (1964b), who designed a general way to turn metric multivariate analysis techniques into non-metric ones. In fact, Kruskal applied the basic methodology developed for multidimensional scaling to linear models in Kruskal (1965), and to principal component analysis in Kruskal and Shepard (1974) (which was actually written around 1965 as well). In parallel developments closely related nonmetric methods were developed by Roskam (1968) and by Guttman and Lingoes (see Lingoes (1973)).\nThe Gifi project took its inspiration from Kruskal, but perhaps even more from Guttman (1941) (and to a lesser extent from the optimal scaling work of Fisher, see Gower (1990)). Guttman’s quantification method, which later became known as multiple correspondence analysis, was merged with linear and nonlinear principal component analysis in the HOMALS/PRINCALS techniques and programs (De Leeuw and Rijckevorsel (1980)). The MVAOS loss function that was chosen ultimately, for example in the work of Burg, De Leeuw, and Verdegaal (1988), had been used earlier by Carroll (1968) in multi-set canonical correlation analysis of numerical variables.\nA project similar to ALSOS/Gifi was ACE, short for Alternating Conditional Expectations. The ACE method for regression was introduced by Breiman and Friedman (1985) and the ACE method for principal component analysis by Koyak (1987). Both techniques use the same ALS block relaxation methods, but instead of projecting on a cone or subspace of possible transformation, they apply a smoother (typically Friedman’s supersmoother) to find the optimal transformation. This implies that the method is intended primarily for continuous variables, and that the convergence properties of the ACE algorithm are more complicated than those of a proper ALS algorithms.\nAn even more closely related project, by Winsberg and Ramsay, uses the cone of I-splines (integrated B-splines) to define the optimal transformations. The technique for linear models is in Winsberg and Ramsay (1980) and the one for principal component analysis in Winsberg and Ramsay (1983). Again, the emphasis on monotonic splines indicates that continuous variables play a larger role than in the ALSOS or Gifi system.\nSo generally there have been a number of projects over the last 50 years that differ in detail, but apply basically the same methodology (alternating least squares and optimal scaling) to generalize classical MVA techniques. Some of them emphasize transformation of continuous variables, some emphasize quantification of discrete variables. Some emphasize monotonicity, some smoothness. Usually these projects include techniques for regression and principal component analysis, but in the case of Gifi the various forms of correspondence analysis and canonical analysis are also included.\nThe techniques discussed in Gifi (1990), and implemented in the corresponding computer programs, use a particular least squares loss function and minimize it by alternating least squares algorithms. All techniques use what Gifi calls meet loss, which is basically the loss function proposed by Carroll (1968) for multiset canonical correlation analysis. Carroll’s work was extended in Gifi by using optimal scaling to transform or quantify variables coded with indicators, and to use constraints on the parameters to adapt the basic technique, often called homogeneity analysis, to different classical MVA techniques.\nThere have been various extensions of the classical Gifi repertoire by adding techniques that do not readily fit into meet loss. Examples are path analysis (Coolen and De Leeuw (1987)), linear dynamic systems (Bijleveld and De Leeuw (1991)), and factor analysis (De Leeuw (2004)). But adding these techniques does not really add up to a new framework.\nSomewhat more importantly, De Leeuw and Rijckevorsel (1988) discuss various ways to generalize meet loss by using fuzzy coding. Transformations are no longer step functions, and coding can be done with fuzzy indicators, such as B-spline bases. This makes it easier to deal with variables that have many ordered categories. Although this is a substantial generalization the basic framework remains the same.\nOne of the outgrowths of the Gifi project was the aspect approach, first discussed systematically by De Leeuw (1988), and implemented in the R package aspect by Mair and De Leeuw (2010). In its original formulation it uses majorization to optimize functions defined on the space of correlation matrices, where the correlations are computed over transformed variables, coded by indicators. Thus we optimize aspects of the correlation matrix over transformations of the variables. The aspect software was recently updated to allow for B-spline transformations (De Leeuw (2015)). Many different aspects were implemented, based on eigenvalues, determinants, multiple correlations, and sums of powers of correlation coefficients. Unformately, aspects defined in terms of canonical correlations, or generalized canonical correlations, were not covered. Thus the range of techniques covered by the aspect approach has multiple regression and principal component analysis in common with the range of the Gifi system, but is otherwise disjoint from it.\nIn De Leeuw (2004) a particular correlation aspect was singled out that could bridge the gap between the aspect approach and the Gifi approach, provided orthoblocks of transformations were introduced. This is combined with the notion of copies, introduced in De Leeuw (1984), to design a new class of techniques that encompasses all of Gifi and that brings generalized canonical correlation analysis in the aspect framework. Thus correlation aspects, and the majorization algorithms to optimize them, are now a true generalization of the Gifi system.\n*** This is the system we discuss in this book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#beyond-gifi",
    "href": "intro.html#beyond-gifi",
    "title": "1  Introduction",
    "section": "",
    "text": "Bijleveld, C. C. J. H., and J. De Leeuw. 1991. “Fitting Longitudinal Reduced-Rank Regression Models by Alternating Least Squares.” Psychometrika 56 (3): 433–47.\n\n\nBock, R. D. 1960. “Methods and Applications of Optimal Scaling.” Psychometric Laboratory Report 25. Chapell Hill, N.C.: L.L. Thurstone Psychometric Laboratory, University of North Carolina.\n\n\nBreiman, L., and J. H. Friedman. 1985. “Estimating Optimal Transformations for Multiple Regression and Correlation.” Journal of the American Statistical Association 80: 580–619.\n\n\nBurg, E. Van der, J. De Leeuw, and R. Verdegaal. 1988. “Homogeneity Analysis with K Sets of Variables: An Alternating Least Squares Approach with Optimal Scaling Features.” Psychometrika 53: 177–97.\n\n\nCarroll, J. D. 1968. “A Generalization of Canonical Correlation Analysis to Three or More Sets of Variables.” In Proceedings of the 76th Annual Convention of the American Psychological Association, 227–28. Washington, D.C.: American Psychological Association.\n\n\nCoolen, H., and J. De Leeuw. 1987. “Least Squares Path Analysis with Optimal Scaling.” Research Report RR-87-03. Leiden, The Netherlands: Department of Data Theory FSW/RUL.\n\n\nCordier, B. 1965. “L’Analyse Factorielle des Correspondances.” {Th\\`ese de Troisieme Cycle}, Université de Rennes.\n\n\nDe Leeuw, J. 1968a. “Canonical Discriminant Analysis of Relational Data.” Research Note 007-68. Department of Data Theory FSW/RUL. https://jansweb.netlify.app/publication/deleeuw-r-68-e/deleeuw-r-68-e.pdf.\n\n\n———. 1968b. “Nonmetric Discriminant Analysis.” Research Note 06-68. Department of Data Theory, University of Leiden.\n\n\n———. 1974. Canonical Analysis of Categorical Data. Leiden, The Netherlands: Psychological Institute, Leiden University.\n\n\n———. 1983. “On the Prehistory of Correspondence Analysis.” Statistica Neerlandica 37: 161–64.\n\n\n———. 1984. “The Gifi System of Nonlinear Multivariate Analysis.” In Data Analysis and Informatics, edited by E. Diday et al. Vol. III. Amsterdam: North Holland Publishing Company.\n\n\n———. 1988. “Multivariate Analysis with Optimal Scaling.” In Proceedings of the International Conference on Advances in Multivariate Statistical Analysis, edited by S. Das Gupta and J. K. Ghosh, 127–60. Calcutta, India: Indian Statistical Institute.\n\n\n———. 2004. “Least Squares Optimal Scaling of Partially Observed Linear Systems.” In Recent Developments in Structural Equation Models, edited by K. van Montfort, J. Oud, and A. Satorra. Dordrecht, Netherlands: Kluwer Academic Publishers.\n\n\n———. 2015. “Aspects of Correlation Matrices.” https://jansweb.netlify.app/publication/deleeuw-e-15-b/deleeuw-e-15-b.pdf.\n\n\nDe Leeuw, J., and J. L. A. Van Rijckevorsel. 1980. “HOMALS and PRINCALS: Some Generalizations of Principal Components Analysis.” In Data Analysis and Informatics. Amsterdam: North Holland Publishing Company.\n\n\n———. 1988. “Beyond Homogeneity Analysis.” In Component and Correspondence Analysis, edited by J. L. A. Van Rijckevorsel and J. De Leeuw, 55–80. Chichester, England: Wiley.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nGower, J. C. 1990. “Fisher’s Optimal Scores and Multiple Correspondence Analysis.” Biometrics 46: 947–61.\n\n\nGower, J. C., and D. J. Hand. 1996. Biplots. Monographs on Statistics and Applied Probability 54. Chapman & Hall.\n\n\nGower, J. C., N. J. Le Roux, and S. Gardner-Lubbe. 2015. “Biplots: Quantitative Data.” WIREs Computational Statistics 7: 42–62.\n\n\n———. 2016. “Biplots: Qualitative Data.” WIREs Computational Statistics 8: 82–111.\n\n\nGuttman, L. 1941. “The Quantification of a Class of Attributes: A Theory and Method of Scale Construction.” In The Prediction of Personal Adjustment, edited by P. Horst, 321–48. New York: Social Science Research Council.\n\n\nHill, M. O. 1974. “Correspondence Analysis: a Neglected Multivariate Method.” Applied Statistics 23: 340–54.\n\n\nKoyak, R. 1987. “On Measuring Internal Dependence in a Set of Random Variables.” Annals of Statistics 15: 1215–28.\n\n\nKruskal, J. B. 1964a. “Multidimensional Scaling by Optimizing Goodness of Fit to a Nonmetric Hypothesis.” Psychometrika 29: 1–27.\n\n\n———. 1964b. “Nonmetric Multidimensional Scaling: a Numerical Method.” Psychometrika 29: 115–29.\n\n\n———. 1965. “Analysis of Factorial Experiments by Estimating Monotone Transformations of the Data.” Journal of the Royal Statistical Society B27: 251–63.\n\n\nKruskal, J. B., and R. N. Shepard. 1974. “A Nonmetric Variety of Linear Factor Analysis.” Psychometrika 39: 123–57.\n\n\nLingoes, J. C. 1973. The Guttman-Lingoes Nonmetric Program Series. Mathesis Press.\n\n\nMair, P., and J. De Leeuw. 2010. “A General Framework for Multivariate Analysis with Optimal Scaling: The r Package Aspect.” Journal of Statistical Software 32 (9): 1–23.\n\n\nRoskam, E. E. 1968. “Metric Analysis of Ordinal Data in Psychology.” PhD thesis, University of Leiden.\n\n\nTanaka, Y. 1979. “Review of the Methods of Quantification.” Environmental Health Perspectives 32: 113–23.\n\n\nWinsberg, S., and J. O. Ramsay. 1980. “Monotone Transformations to Additivity.” Biometrika 67: 669–74.\n\n\n———. 1983. “Monotone Spline Transformations for Dimension Reduction.” Psychometrika 48: 575–95.\n\n\nYoung, F. W. 1981. “Quantitative Analysis of Qualitative Data.” Psychometrika 46: 357–88.\n\n\nYoung, F. W., J. De Leeuw, and Y. Takane. 1980. “Quantifying Qualitative Data.” In Similarity and Choice. Papers in Honor of Clyde Coombs, edited by E. D. Lantermann and H. Feger. Bern: Hans Huber.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "2  Coding and Transformations",
    "section": "",
    "text": "2.1 Variables and Multivariables\nIn the multivariate analysis techniques presented in this book the data are measurements or classifications of \\(n\\) objects by \\(m\\) variables. Perhaps it is useful to insert some definitions here. A variable is a function that maps a domain of objects to a range of values. Domains are finite. The elements of the domain can be individuals, animals, plants, time points, locations, and so on. It is useful to distinguish the codomain (or range) of a variable and its image. The codomain of a variable can be the real numbers, but the image always is a finite set, the actual values the variable assumes on the domain. A multivariable is a sequence of variables defined on the same domain, with possibly different codomains. Multivariables are implemented in R as dataframes. Variables can have a finite codomain, which can be either ordered or unordered. This corresponds with a factor or an ordered factor in R. MVAOS techniques quantify factors, replacing the values in the image by real numbers. If the variables are real-valued to start with we replace real numbers by other real numbers and we transform instead of quantify. The distinction between quantification and transformation is somewhat fluid, because the image of a variable is always finite and thus, in a sense, all variables are categorical (a point also emphasized, for example, in Holland (1979)).\nAlthough the variables in a multivariable have the same domain, there can be different numbers of missing data for different variables. We handle this in the same way as R, by adding NA to the range of all variables. In this context it is also useful to define latent or unobserved variables. These are variables for which all values are missing, i.e. for which the image only contains NA. At first thought it seems somewhat perverse to have such completely missing variables, but think of examples such as principal components, factor scores, or error terms in linear models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#induced-correlations-and-aspects",
    "href": "coding.html#induced-correlations-and-aspects",
    "title": "2  Coding and Transformations",
    "section": "2.2 Induced Correlations and Aspects",
    "text": "2.2 Induced Correlations and Aspects\nIf all categorical variables are quantified and all numerical variables are transformed we can compute the induced correlation matrix of the transformed and quantified variables. In the forms of MVAOS we consider in this book the statistics we compute, except for the transformations themselves, are usually functions of this induced correlation matrix. This means that they are functions of the second order relations between the variables, or, in order words, they are joint bivariate. Higher order moments and product moments are ignored. Different multivariate distributions with the same bivariate marginals will give the same MVAOS results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#transformed-variables",
    "href": "coding.html#transformed-variables",
    "title": "2  Coding and Transformations",
    "section": "2.3 Transformed Variables",
    "text": "2.3 Transformed Variables\nThe data are collected in the \\(n\\times m\\) matrix \\(H\\), which codes the observations on the \\(m\\) variables. MVAOS does not operate on the data directly, but on transformations or quantifications of the variables. Choosing a transformation to minimize a loss function is known as optimal scaling. Clearly this so-called optimality is only defined in terms of a specific loss function, with specific constraints. Different constraints and different loss functions will lead to different optimal transformations.\nLet us define the types of transformations we are interested in. The \\(n\\times m\\) matrix of transformed variables \\(H\\) has columns \\(h_j\\), which are constrained by \\(h_j=G_jz_j\\), where \\(G_j\\) is a given matrix defining the basis for variable \\(j\\). In addition we require \\(h_j\\in\\mathcal{C}_j\\) and \\(h_j\\in\\mathcal{S}\\), where \\(\\mathcal{C}_j\\) is a cone of transformations and \\(\\mathcal{S}\\) is the unit sphere in \\(\\mathbb{R}^n\\). This will be discussed in more detail in later sections, but for the time being think of the example in which \\(h_j\\) is required to be a (centered and normalized) monotone polynomial function of the image values of variable \\(j\\). The whole of \\(\\mathbb{R}^n\\) and a single point in \\(\\mathbb{R}^n\\) are both special cases of these normalized cones. It is important, especially for algorithm construction, that the restrictions are defined for each variable separately. An exception to this rule is the orthoblock, using terminology from De Leeuw (2004), which requires that all or some of the columns of \\(H\\) are not only normalized but also orthogonal to each other. Clearly a normalized variable is an orthoblock of size one.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#bases",
    "href": "coding.html#bases",
    "title": "2  Coding and Transformations",
    "section": "2.4 Bases",
    "text": "2.4 Bases\nIn earlier MVAOS work, summarized for example in Gifi (1990) or Michailidis and De Leeuw (1998), the basis matrices \\(G_j\\) were binary zero-one matrices, indicating category membership. These matrices are also known as indicator matrices. The same is true for the software in IBM SPSS Categories (Meulman and Heiser 2012) or in the R package homals (De Leeuw and Mair 2009). In this paper we extend the current MVAOS software using B-spline bases, which provide a form of fuzzy non-binary coding suitable for both categorical and numerical variables (Van Rijckevorsel and De Leeuw 1988). B-spline basis were already discussed for some special cases in De Leeuw, Rijckevorsel, and Wouden (1981) and Gifi (1990), but corresponding easily accessible software was never released.\nIn this book we continue to use the term indicators for bases. Thus bases \\(G_j\\) must be non-negative, with rows that add up to one. If there is only one non-zero entry in each row, which of course is then equal to one, the indicator is crisp, otherwise it is fuzzy. B-spline bases are the prime example of fuzzy indicators, but other examples are discussed in Van Rijckevorsel and De Leeuw (1988). Only B-spline bases are implemented in our software, however.\nNote that the identity matrix is a crisp indicator. This is of importance in connection with missing data and orthoblocks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#copies-and-rank",
    "href": "coding.html#copies-and-rank",
    "title": "2  Coding and Transformations",
    "section": "2.5 Copies and Rank",
    "text": "2.5 Copies and Rank\nWithin a block there can be more than one version of the same variable. These multiple versions are called copies. They were first introduced into the Gifi framework by De Leeuw (1984). Since MVAOS transforms variables, having more than one copy is not necessarily redundant, because different copies can and will be transformed differently. As a simple example of copies, think of using different monomials or orthogonal polynomials of a single variable \\(x\\) in a polynomial regression. The difference between copies and simply including a variable more than once is that copies have the same basis \\(G_j\\).\nIn the Gifi algorithms copies of a variable are treated in exactly the same way as other variables. The notion of copies replaces the notion of the rank of a quantification used in traditional Gifi, which in turn generalizes the distinction between single and multiple quantifications. A single variable has only one copy in its block, a multiple variable has the maximum number of copies.\nIn our software the copies of a variable by definition have the same basis. It is possible, of course, to include the same variable multiple times, but with different bases. This must be done, however, at the input level. In terms of the structures defined in the software, a gifiVariable can have multiple copies but it only has one basis. If there is more than one basis for a variable, then we need to define an additional gifiVariable. Also note that copies of a variable are all in the same block. If you want different versions of a variable in different blocks, then that requires you to create different gifiVariables.\nDefining copies is thus basically a coding problem. It can be handled simply by adding a variable multiple times to a data set, and giving each variable the same bases. In our algorithm we use the fact that copies belong to the same variable to create some special shortcuts and handling routines.\nOrdinality restrictions on variables with copies require some special attention. In our current implementation we merely require the first copy to be ordinal with the data, the other copies are not restricted. Once again, if you want ordinal restrictions on all copies you need to create separate gifiVariables for each copy.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#orthoblocks",
    "href": "coding.html#orthoblocks",
    "title": "2  Coding and Transformations",
    "section": "2.6 Orthoblocks",
    "text": "2.6 Orthoblocks\nIf a variable has more than one copy, then we require without loss of generality that the transformations are orthogonal.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#constraints",
    "href": "coding.html#constraints",
    "title": "2  Coding and Transformations",
    "section": "2.7 Constraints",
    "text": "2.7 Constraints\nAs discussed earlier, each variable has a cone of transformations associated with it, and we optimize over these transformations. In ALSOS and classical Gifi the three type of transformation cones considered are nominal, ordinal, and numerical. Our use of B-splines generalizes this distinction, because both numerical and nominal can be implemented using splines. What remains is the choice for the degree of the spline and the location of the knots.\nChoice of degree and knots is basically up to the user, but the programs have some defaults. In most cases the default is to use crisp indicators with knots at the data points. Of course for truly categorical variables (i.e. for factors in R) crisp indicators are simply constructed by using the levels of the factor. We include some utilities to place knots at percentiles, or equally spaced on the range, or to have no interior knots at all (in which case we fit polynomials).\nAnd finally the user decides, for all variables, if she wants the transformations (step functions, splines, and polynomials) to be monotonic with the data. Default is not requiring monotonicity.\nNote that we require the spline to be monotonic in the non-missing data points – this does not mean the spline is monotonic outside the range of the data (think, for example, of a quadratic polynomial), it does not even mean the spline is monotonic between data points. This makes our spline transformations different from the integrated B-splines, or I-splines, used by Winsberg and Ramsay (1983), which are monotone on the whole real line. Because each variable has a finite image we are not really fitting a spline, we are fitting a number of discrete points that are required to be on a spline, and optionally to be monotonic with the data. In Winsberg and Ramsay (1983) the requirement is that the fitted points are on an I-spline, which automatically makes them monotonic with the data. Clearly our approach is the less restrictive one.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#missing-data",
    "href": "coding.html#missing-data",
    "title": "2  Coding and Transformations",
    "section": "2.8 Missing Data",
    "text": "2.8 Missing Data\nThe utility makeMissing() expands the basis for the non-missing data in various ways. Option “m” (for “multiple”) is the default. It replaces the basis with the direct sum of the non-missing basis and an identity matrix for the missing elements. Option “s” (for “single”) adds a single binary column to the basis indicating which elements are missing. Option “a” (for “average”) codes missing data by having all the elements in rows of the basis corresponding with missing data equal to one over the number of rows. With all three options the basis remains an indicator. Some of these options make most sense in the context of crisp indicators, where they are compared in Meulman (1982).\nSo suppose the data are\n\n\n     [,1] \n[1,] -0.50\n[2,]    NA\n[3,]  0.75\n[4,]  0.99\n[5,]    NA\n\n\nCreate a basis for the non-missing values with\n\nmprint(basis &lt;- bsplineBasis(x[which(!is.na(x))],1,c(-1,0,1)))\n\n     [,1]  [,2]  [,3] \n[1,]  0.50  0.50  0.00\n[2,]  0.00  0.25  0.75\n[3,]  0.00  0.01  0.99\n\n\nThe three different completion options for missing data give\n\nmprint (makeMissing (x, basis, missing = \"m\"))\n\n     [,1]  [,2]  [,3]  [,4]  [,5] \n[1,]  0.50  0.50  0.00  0.00  0.00\n[2,]  0.00  0.00  0.00  1.00  0.00\n[3,]  0.00  0.25  0.75  0.00  0.00\n[4,]  0.00  0.01  0.99  0.00  0.00\n[5,]  0.00  0.00  0.00  0.00  1.00\n\nmprint (makeMissing (x, basis, missing = \"s\"))\n\n     [,1]  [,2]  [,3]  [,4] \n[1,]  0.50  0.50  0.00  0.00\n[2,]  0.00  0.00  0.00  1.00\n[3,]  0.00  0.25  0.75  0.00\n[4,]  0.00  0.01  0.99  0.00\n[5,]  0.00  0.00  0.00  1.00\n\nmprint (makeMissing (x, basis, missing = \"a\"))\n\n     [,1]  [,2]  [,3] \n[1,]  0.50  0.50  0.00\n[2,]  0.33  0.33  0.33\n[3,]  0.00  0.25  0.75\n[4,]  0.00  0.01  0.99\n[5,]  0.33  0.33  0.33\n\n\nThe default option for missing data in the previous version of the Gifi system was “missing data deleted”, which involves weighting the rows in the loss functions by the number of non-missing data in that row. This leads to some complications, and consequently we have no option “d” in this version of Gifi.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#active-and-passive-variables",
    "href": "coding.html#active-and-passive-variables",
    "title": "2  Coding and Transformations",
    "section": "2.9 Active and Passive Variables",
    "text": "2.9 Active and Passive Variables\nIf a variable is passive (or supplementary) it is incorporated in the analysis, but it does not contribute to the loss. Thus an analysis that leaves the passive variables out will give the same results for the active variables. Passive variables are transformed like all the others, but they do not contribute to the block scores, and thus not to the loss. They have category quantifications and scores, and can be used in the corresponding plots.\nIf all variables in a block are passive, then the whole block does not contribute to the loss. This happens specifically for singletons, if the single variable in the block is passive.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "coding.html#interactive-coding",
    "href": "coding.html#interactive-coding",
    "title": "2  Coding and Transformations",
    "section": "2.10 Interactive Coding",
    "text": "2.10 Interactive Coding\nOne of the major contributions of Analyse des Données is the emphasis on coding, which in our context can be defined as choosing how to represent the raw data of an experiment in an actual data frame (and, to a lesser extent, how to choose blocks, number of copies, dimensionality, degrees, and knots). In the section we discuss one important coding variation. Suppose we have \\(n\\) observations on two factors, one with \\(p\\) levels and one with \\(q\\) levels. Then the data can be coded as \\(n\\) observations on one factor with \\(p\\times q\\) levels, and we can construct a corresponding crisp indicator. The same reasoning applies to more than two categorical variables, which we can always code interactively. It also applies to bases for numerical variables, where we can define an interactive basis by using products of columns from the bases of each of the variables.\nIf \\(G=\\{g_{is}\\}\\) and \\(H=\\{h_{it}\\}\\) are two indicators of dimensions \\(n\\times m_g\\) and \\(n\\times m_h\\), then the \\(n\\times m_gm_h\\) matrix with elements {g_{is}h_{it}} is again an indicator: the elements are non-negative, and rows add up to one.\n\nmprint (x &lt;- bsplineBasis (1:9/10, 1, .5))\n\n      [,1]  [,2]  [,3] \n [1,]  1.00  0.00  0.00\n [2,]  0.75  0.25  0.00\n [3,]  0.50  0.50  0.00\n [4,]  0.25  0.75  0.00\n [5,]  0.00  1.00  0.00\n [6,]  0.00  0.75  0.25\n [7,]  0.00  0.50  0.50\n [8,]  0.00  0.25  0.75\n [9,]  0.00  0.00  1.00\n\n\n\nmprint (y &lt;- makeIndicator (c (rep (1, 5), rep (2, 4))))\n\n      [,1]  [,2] \n [1,]  1.00  0.00\n [2,]  1.00  0.00\n [3,]  1.00  0.00\n [4,]  1.00  0.00\n [5,]  1.00  0.00\n [6,]  0.00  1.00\n [7,]  0.00  1.00\n [8,]  0.00  1.00\n [9,]  0.00  1.00\n\n\n\nmprint (makeColumnProduct (list (x, y)))\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6] \n [1,]  1.00  0.00  0.00  0.00  0.00  0.00\n [2,]  0.75  0.00  0.25  0.00  0.00  0.00\n [3,]  0.50  0.00  0.50  0.00  0.00  0.00\n [4,]  0.25  0.00  0.75  0.00  0.00  0.00\n [5,]  0.00  0.00  1.00  0.00  0.00  0.00\n [6,]  0.00  0.00  0.00  0.75  0.00  0.25\n [7,]  0.00  0.00  0.00  0.50  0.00  0.50\n [8,]  0.00  0.00  0.00  0.25  0.00  0.75\n [9,]  0.00  0.00  0.00  0.00  0.00  1.00\n\n\n\n\n\n\nDe Leeuw, J. 1984. “The Gifi System of Nonlinear Multivariate Analysis.” In Data Analysis and Informatics, edited by E. Diday et al. Vol. III. Amsterdam: North Holland Publishing Company.\n\n\n———. 2004. “Least Squares Optimal Scaling of Partially Observed Linear Systems.” In Recent Developments in Structural Equation Models, edited by K. van Montfort, J. Oud, and A. Satorra. Dordrecht, Netherlands: Kluwer Academic Publishers.\n\n\nDe Leeuw, J., and P. Mair. 2009. “Homogeneity Analysis in R: the Package homals.” Journal of Statistical Software 31 (4): 1–21. https://www.jstatsoft.org/v31/i04/.\n\n\nDe Leeuw, J., J. L. A. Van Rijckevorsel, and H. Van der Wouden. 1981. “Nonlinear Principal Component Analysis Using b-Splines.” Methods of Operations Research 43: 379–94.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nHolland, P. W. 1979. “The Tyranny of Continuous Models in a World of Discrete Data.” IHS-Journal 3: 29–42.\n\n\nMeulman, J. J. 1982. Homogeneity Analysis of Incomplete Data. Leiden, The Netherlands: DSWO Press.\n\n\nMeulman, J. J., and W. J. Heiser. 2012. IBM SPSS Categories 21. IBM Corporation.\n\n\nMichailidis, G., and J. De Leeuw. 1998. “The Gifi System for Descriptive Multivariate Analysis.” Statistical Science 13: 307–36.\n\n\nVan Rijckevorsel, J. L. A., and J. De Leeuw, eds. 1988. Component and Correspondence Analysis. Wiley.\n\n\nWinsberg, S., and J. O. Ramsay. 1983. “Monotone Spline Transformations for Dimension Reduction.” Psychometrika 48: 575–95.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding and Transformations</span>"
    ]
  },
  {
    "objectID": "algorithm.html",
    "href": "algorithm.html",
    "title": "3  Algorithm",
    "section": "",
    "text": "3.1 Block Relaxation\nOur task is to minimize \\(\\sigma(H,A)\\) over \\(H\\) and \\(A\\), suitably constrained. Write the constraints as \\(H\\in\\mathcal{H}\\) and \\(A\\in\\mathcal{A}\\). The strategy we use is block relaxation ((deleeuw_B_15?)). Thus we iterate as follows.\nIt is assumed that step 1, updating \\(A\\) for given \\(H\\), can be carried out simply by some form of linear least squares. We assume that for each \\(\\ell\\) there is at least one \\(j\\) such that \\(A_{j\\ell}=I\\). Note that this is the case for MLR, PCA, EFA, and for all Gifi Systems.\nStep 2 is somewhat more intricate, because of the cone restrictions. In partitioned form we can write the loss function as \\[\n\\sigma(H,A)=\\sum_{i=1}^m\\mathbf{tr}\\ H_i'\\sum_{j=1}^mH_j\\sum_{\\ell=1}^LA_{j\\ell}A_{i\\ell}'\n\\]\n\\[\nB_{ij}(A)=\\sum_{\\ell=1}^LA_{j\\ell}A_{i\\ell}'\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm</span>"
    ]
  },
  {
    "objectID": "algorithm.html#block-relaxation",
    "href": "algorithm.html#block-relaxation",
    "title": "3  Algorithm",
    "section": "",
    "text": "Set \\(k=0\\) and start with some \\(H^{(0)}\\).\n\\(A^{(k)}\\in\\amin{A\\in\\mathcal{A}}\\ \\sigma(H^{(k)},A)\\).\n\\(H^{(k+1)}\\in\\amin{H\\in\\mathcal{H}}\\ \\sigma(H,A^{(k)})\\).\nIf converged stop. Else \\(k\\leftarrow k+1\\) and go to step 1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm</span>"
    ]
  },
  {
    "objectID": "algorithm.html#majorization",
    "href": "algorithm.html#majorization",
    "title": "3  Algorithm",
    "section": "3.2 Majorization",
    "text": "3.2 Majorization\n\\[\n\\mathbf{tr}\\ H'HG=\\mathbf{tr}\\ (\\tilde H + (H - \\tilde H))'(\\tilde H + (H - \\tilde H))G\\geq\\\\\\mathbf{tr}\\ \\tilde H'\\tilde HG+2\\mathbf{tr}\\ \\tilde H'(H - \\tilde H)G\n\\]\n\\[\n\\mathbf{tr}\\ H'\\tilde HG(\\tilde H)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm</span>"
    ]
  },
  {
    "objectID": "algorithm.html#alternating-least-squares",
    "href": "algorithm.html#alternating-least-squares",
    "title": "3  Algorithm",
    "section": "3.3 Alternating Least Squares",
    "text": "3.3 Alternating Least Squares\nThe standard way to minimize loss function \\(\\eqref{E:oldloss}\\) is implemented in the OVERALS program Meulman and Heiser (2012). It is also the one used in the homals package (De Leeuw and Mair 2009).\nIn this paper the algorithm is different because we use the loss function \\(\\eqref{E:gifiloss}\\). We still use ALS, which means in this case that we cycle through three substeps in each iteration. We update \\(A\\) for given \\(X\\) and \\(H\\), we then update \\(X\\) for given \\(H\\) and \\(A\\), and finally we update \\(H\\) for given \\(X\\) and \\(A\\). Algorithm A goes as follows.\n\nSet \\(k=0\\) and start with some \\(X^{(0)},H^{(0)},A^{(0)}\\).\n\\(X^{(k+1)}=\\mathbf{ortho}(\\mathbf{center}(H^{(k)}A^{(k)})\\).\nFor \\(j=1,\\cdots,m\\) compute \\(A_j^{(k+1)}=\\{H_j^{(k)}\\}^+X^{(k+1)}\\).\nFor \\(j=1,\\cdots,m\\) and \\(s=1,\\cdots p_j\\) compute \\(h_{js}^{(k+1)}=\\mathbf{proj}_{\\mathcal{K}_{js}\\cap\\mathcal{S}}((X^{(k+1)}-\\sum_{t&lt;s}h_{jt}^{(k+1)}\\{a_{jt}^{(k+1)}\\}'-\\sum_{t&gt;s}h_{jt}^{(k)}\\{a_{jt}^{(k+1)}\\}')a_s^{(k+1)})\\).\nIf converged stop. Else \\(k\\leftarrow k+1\\) and go to step 1.\n\nIn step 1 we use superscript + for the Moore-Penrose inverse. In step 2 the center operator does column centering, the ortho operator finds an orthonormal basis for the column space of its argument.\nThe complicated part is step 4, the optimal scaling, i.e. the updating of \\(H_j\\) for given \\(X\\) and \\(A_j\\). We cycle through the variables in the block, each time projecting a single column on the cone of admissible transformations of the variable, and then normalizing the projection to length one. The target, i.e. the vector we are projecting, is complicated, because the other variables in the same block must be taken into account.\nIn order to simplify the optimal scaling computations within an iteration we can use majorization (deleeuw_B_15?). This has the additional benefit that the optimal scaling step becomes embarassingly parallel. We expand the loss for block \\(j\\) around a previous solution \\(\\tilde H_j\\). \\[\n\\mathbf{SSQ}(X-H_jA_j)=\n\\mathbf{SSQ}(X-\\tilde H_jA_j)-2\\mathbf{tr}\\ (H_j-\\tilde H_j)'(X-\\tilde H_jA_j)A_j'\n+\\mathbf{tr}\\ A_j'(H_j-\\tilde H_j)'(H_j-\\tilde H_j)A_j.\n\\] Now \\[\n\\mathbf{tr}\\ (H_j-\\tilde H_j)A_jA_j'(H_j-\\tilde H_j)'\\leq\\kappa_j\\ \\mathbf{tr}\\ (H_j-\\tilde H_j)'(H_j-\\tilde H_j),\n\\] where \\(\\kappa_j\\) is the largest eigenvalue of \\(A_j'A_j\\). Thus \\[\n\\mathbf{SSQ}(X-H_jA_j)\\leq\\mathbf{SSQ}(X-\\tilde H_jA_j)+\\kappa_j\\ \\mathbf{SSQ}(H_j-U_j)-\\frac{1}{\\kappa_j}\\ \\mathbf{SSQ}((X-\\tilde H_jA_j)A_j'),\n\\] where \\(U_j\\) is the target \\[\nU_j=\\tilde H_j+\\frac{1}{\\kappa_j}(X-\\tilde H_jA_j)A_j'.\\tag{3}\n\\] It follows we can update the optimal scaling of the variables by projecting the columns of \\(U_j\\) on their respective cones and then normalizing. See De Leeuw (1975) for results on normalized cone regression. This can be done for all variables in the block separately, without taking any of the other variables in the block (or in any of the other blocks) into account. Thus the optimal scaling is easy to parallellize. The resulting algorithm B is as follows.\n\nSet \\(k=0\\) and start with some \\(X^{(0)},H^{(0)},A^{(0)}\\).\n\\(X^{(k+1)}=\\mathbf{ortho}(\\mathbf{center}(H^{(k)}A^{(k)})\\).\nFor \\(j=1,\\cdots,m\\) compute \\(A_j^{(k+1)}=\\{H_j^{(k)}\\}^+X^{(k+1)}\\).\nFor \\(j=1,\\cdots,m\\) compute \\(U_j^{(k+1)}=H_j^{(k)}+\\frac{1}{\\kappa_j}(X^{(k+1)}-H_j^{(k)}A_j^{(k+1)})\\{A_j^{(k+1)}\\}'\\) and for \\(s=1,\\cdots p_j\\) compute \\(h_{js}^{(k+1)}=\\mathbf{proj}_{\\mathcal{K}_{js}\\cap\\mathcal{S}}(u_{js}^{(k+1)})\\).\nIf converged stop. Else \\(k\\leftarrow k+1\\) and go to step 1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm</span>"
    ]
  },
  {
    "objectID": "algorithm.html#implementation-details",
    "href": "algorithm.html#implementation-details",
    "title": "3  Algorithm",
    "section": "3.4 Implementation Details",
    "text": "3.4 Implementation Details\nIf we follow the ALS strategy strictly the \\(\\mathbf{ortho}()\\) operator should be implemented using Procrustus rotation (Gibson 1962). Thus if \\(Z=K\\Lambda L'\\) is the singular value decomposition of \\(X\\), then \\(\\mathbf{ortho}(Z)=KL'\\). Note, however, that any other basis for the column space of \\(Z\\) merely differs from the Procrustus basis by a rotation. And this rotation matrix will carry unmodified into the upgrade of \\(A_j\\) in step 2 of the algorithm, and thus after steps 1 and 2 the loss will be the same, no matter which rotation we select. In our algorithm we use the QR decomposition to find the basis, using the Gram-Schmidt code from De Leeuw (2015a).\nIn actual computation we column-center the basis and compute a full rank QR decomposition, using the code in De Leeuw (2015a). Thus \\(G_\\ell=Q_\\ell R_\\ell\\),\nWe implement the cone restrictions by the constraints \\(h_{js}=G_{js}z_s\\) in combination with \\(T_{js}h_{js}\\geq 0\\). Thus the transformed variables must be in the intersection of the subspace spanned by the columns of the transformation basis \\(G_{js}\\) and the polyhedral convex cones of all vectors \\(h\\) such that \\(T_{js}h\\geq 0\\). We suppose that all columns of the \\(G_{js}\\) add up to zero, and we require, in addition, the normalization \\(SSQ(h_{js})=1\\).\nWe use the code described in De Leeuw (2015b) to generate B-spline bases. Note that for coding purposes binary indicators are B-splines of degree zero, while polynomials are B-splines without interior knots. We include the utility functions to generate lists of knots. There is knotsQ() for knots at the quantiles, knotsR() for knots equally spaced on the range, knotsD() for knots at the data points, and knotsE() for no interior knots. Also note that binary indicators can be created for qualitative non-numerical variables, for which B-splines are not defined. We have added the option using degree -1 to bypass the B-spline code and generate an indicator matrix, using the utility makeIndicator(). Note that 'makeIndicator(foo) is equivalent to bsplineBasis(foo, degree = 0, innerknots = sort(unique(foo))). Throughout we first orthonormalize the basis matrices \\(G_{js}\\), using the Gram-Schmidt code from De Leeuw (2015a).\nThe matrices \\(T_{js}\\) in the homogeneous linear inequality restrictions that define the cones \\(\\mathcal{K}_{js}\\) can be used to define monotonicity or convexity of the resulting transformations. In the current implementation we merely allow for monotonicity, which means the \\(T_{js}\\) do not have to be stored. The transformations for each variable can be restricted to be increasing, or they can be unrestricted. By using splines without interior knots we allow in addition for polynomial transformations, which again can be restricted to be either monotonic or not. Note that it is somewhat misleading to say we are fitting monotone splines or polynomials, we are mainly requiring monotonicity at the data points.\nIf there are multiple copies of a variable in a block then requiring the transformation to be ordinal means that we want the transformation of the first copy to be monotonic. The transformations of the other copies are not constrained to be monotonic. If you want all copies to be transformed monotonically, you have to explicitly introduce them as separate variables.\nFor variables with copies there is yet another complication. For copies we have \\(H_jA_j=G_j(Z_jA_j)=G_jY_j\\). If we require monotonicity in MVAOS we constrain a column of \\(H_j\\) (in fact, the first one) to be monotonic. In classic Gifi, in which the \\(G_j\\) are binary indicators, we constrain the first column of \\(Y_j\\), which automatically implies the first column of \\(G_jY_j\\) is monotonic as well. In previous Gifi work with B-splines, we also constrained the first column of \\(Y_j\\), which again implied the first column of \\(G_jY_j\\) was monotnic as well. But in our current MVAOS implementation monotonicity of the first column of \\(H_j\\) does not imply monotonicity of the first column of \\(H_jA_j\\), even if the basis \\(G_j\\) is a binary indicator. This discrepancy between the old and the new Gifi only comes into play for ordinal variables with multiple copies.\nMissing data are incorporated in the definition of the cones of transformations by using a \\(G_{js}\\) which is the direct sum of a spline basis for the non-missing and an identity matrix for the missing data. This is called missing data multiple in Gifi (1990). There are no linear inequality restrictions on the quantifications of the missing data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm</span>"
    ]
  },
  {
    "objectID": "algorithm.html#wrappers",
    "href": "algorithm.html#wrappers",
    "title": "3  Algorithm",
    "section": "3.5 Wrappers",
    "text": "3.5 Wrappers\nThe homals() implementation in De Leeuw and Mair (2009) is a single monolithic program in R, which specializes to the various MVAOS techniques by a suitable choice of its parameters. This approach has some disadvantages. If we want principal component analysis, we already know all blocks are singletons. If we want multiple correspondence analysis we know each variable has \\(p\\) copies. If we want multiple regression, we know there are two blocks, and one is a singleton. So it is somewhat tedious to specify all parameters all of the time. Also, some of the output, graphical and otherwise, is specific to a particular technique. For regression we want residuals and fitted values, in canonical analysis we want block scores and loadings. And, more generally, we may want the output in a form familiar from the classical MVA techniques. It is indeed possible to transform the homals() output to more familar forms (De Leeuw (2009)), but this requires some extra effort.\nIn this book we go back to the original approach of Gifi (1990) and write separate programs for nonlinear versions principal component analysis, multiple regression, canonical analysis, discriminant analysis, and so on.\nThese programs, now written in R and no longer in FORTRAN, are wrappers for the main computational core, the program gifiEngine(). The wrappers, which have the familiar names morals(), corals(), princals(), homals(), criminals(), overals(), primals(), and canals(), create a gifi object from the data and parameters, and then pass this to gifiEngine(). Computations are itereated to convergence, and result are stored in a xGifi object. Then the output is transformed to a format familiar from the corresponding technique from classical MVA. Each wrapper foo returns a structure of class foo.\nThis modular approach saves code, because both makeGifi() and gifiEngine() are common to all programs. It also makes it comparatively easy to add new wrappers not currently included, possibly even contributed by others.\nAlthough we like the above quotation from Hill (1990), it is not quite accurate. Our current generation of wrappers can use B-spline bases, it can use an arbitrary number of copies of a variable, and each copy can be either categorical, ordinal, polynomial, or splinical. Thus, even more so than the original gifi programs, we have a substantial generalization of the classical techniques, not merely a sequence of synonyms.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm</span>"
    ]
  },
  {
    "objectID": "algorithm.html#structures",
    "href": "algorithm.html#structures",
    "title": "3  Algorithm",
    "section": "3.6 Structures",
    "text": "3.6 Structures\nThe computations are controlled by the arguments to the wrappers. These arguments are used to construct three structures: the gifi, the gifiBlock, and the gifiVariable. A gifi is just a list of gifiBlocks, and a gifiBlock is a list of gifiVariables. This reflects the partitioning of the variables into blocks. A gifiVariable contains a great deal of information about the variable. The function makeGifiVariable() is a constructor that returns a structure of class gifiVariable. The contents of a gifiVariable remain the same throughout the computations.\n\n    return (structure (\n      list (\n        data = data,\n        basis = basis,\n        qr = qr,\n        copies = copies,\n        degree = degree,\n        ties = ties,\n        missing = missing,\n        ordinal = ordinal,\n        active = active,\n        name = name,\n        type = type\n      ),\n      class = \"gifiVariable\"\n    ))\n\nThere are three corresponding structures containing initial and intermediate results, and eventually output, the xGifi, xGifiBlock, and xGifiVariable. Again, an xGifi is a list of xGifiBlocks, and an xGifiBlock is a list of xGifiVariables. The constructor for an xGifiVariable returns an object of class xGifiVariable, which contains the elements that are updated in each iteration during the computations. There is an xGifiVariable for both active and passive variables.\n\n return (structure (\n    list(\n      transform = transform,\n      weights = weights,\n      scores = scores,\n      quantifications = quantifications\n    ),\n    class = \"xGifiVariable\"\n  ))\n\n\n\n\n\nBurg, E. Van der, J. De Leeuw, and R. Verdegaal. 1988. “Homogeneity Analysis with K Sets of Variables: An Alternating Least Squares Approach with Optimal Scaling Features.” Psychometrika 53: 177–97.\n\n\nDe Leeuw, J. 1975. “A Normalized Cone Regression Approach to Alternating Least Squares Algorithms.” Department of Data Theory FSW/RUL. https://jansweb.netlify.app/publication/deleeuw-u-75-a/deleeuw-u-75-a.pdf.\n\n\n———. 1994. “Block Relaxation Algorithms in Statistics.” In Information Systems and Data Analysis, edited by H. H. Bock, W. Lenski, and M. M. Richter, 308–24. Berlin: Springer Verlag. https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf.\n\n\n———. 2009. “Regression, Discriminant Analysis, and Canonical Analysis with homals.” Preprint Series 562. Los Angeles, CA: UCLA Department of Statistics.\n\n\n———. 2015a. “Exceedingly Simple b-Spline Code.”\n\n\n———. 2015b. “Regression with Linear Inequality Restrictions on Predicted Values.”\n\n\nDe Leeuw, J., and P. Mair. 2009. “Homogeneity Analysis in R: the Package homals.” Journal of Statistical Software 31 (4): 1–21. https://www.jstatsoft.org/v31/i04/.\n\n\nGibson, W. A. 1962. “On the Least Squares Orthogonalization of an Oblique Transformation.” Psychometrika 27: 193–95.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nHeiser, W. J. 1995. “Convergent Computing by Iterative Majorization: Theory and Applications in Multidimensional Data Analysis.” In Recent Advantages in Descriptive Multivariate Analysis, edited by W. J. Krzanowski, 157–89. Oxford: Clarendon Press.\n\n\nHill, M. O. 1990. “Review of A. Gifi, Multivariate Analysis.” Journal of Ecology 78 (4): 1148–49.\n\n\nLange, K., D. R. Hunter, and I. Yang. 2000. “Optimization Transfer Using Surrogate Objective Functions.” Journal of Computational and Graphical Statistics 9: 1–20.\n\n\nMeulman, J. J., and W. J. Heiser. 2012. IBM SPSS Categories 21. IBM Corporation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algorithm</span>"
    ]
  },
  {
    "objectID": "mca.html",
    "href": "mca.html",
    "title": "4  Multiple Correspondence Analysis and homals()",
    "section": "",
    "text": "4.1 Introduction\nSuppose all basis matrices \\(G_{j\\ell}\\) in block \\(j\\) are the same, say equal to \\(G_j\\). Then the block scores \\(H_jA_j\\) are equal to \\(G_jZ_jA_j\\), which we can write simply as \\(G_jY_j\\). Thus loss must be minimized over \\(X\\) and the \\(Y_j\\).\nIf all \\(G_j\\) are binary indicators of categorical variables, and the \\(m\\) blocks are all of span one, then MVAOS is multiple correspondence analysis (MCA). The block scores \\(G_jY_j\\) are \\(k_j\\) different points in \\(\\mathbb{R}^p\\), with \\(k_j\\) the number of categories of the variable, which is usually much less than \\(n\\). The plot connecting the block scores to the object scores is called the star plot of the variable. If \\(k_j\\) is much smaller than \\(n\\) a star plot will connect all object scores to their category centroids, and the plot for a block (i.e. a variable) will show \\(k_j\\) stars. Since loss \\(\\sigma\\) is equal to the sum of squared distances between object scores and block scores, we quantify or transform variables so that stars are small.\nIn our MVAOS MCA function homals() we allow for B-spline bases and for monotonicity restrictions. The input data (as for all MVAOS programs) needs to be numeric, and we included a small utility function makeNumeric() that can be used on data frames, factors, and character variables to turn them into numeric matrices. All other arguments to the function have default values.\nhomals &lt;-\n  function (data,\n            knots = knotsD (data),\n            degrees = rep (-1, ncol (data)),\n            ordinal = rep (FALSE, ncol (data)),\n            ndim = 2,\n            ties = \"s\",\n            missing = \"m\",\n            names = colnames (data, do.NULL = FALSE),\n            itmax = 1000,\n            eps = 1e-6,\n            seed = 123,\n            verbose = FALSE)\nThe output is a structure of class homals, i.e. a list with a class attributehomals. The list consists of transformed variables (in xhat), their correlation (in rhat), the objectscores (in objectscores), the blockscores (in blockscores, which is itself a list of length number of variables), the discrimination matrices (in dmeasures, a list of length number of variables), their average (in lambda), the weights (in a), the number of iterations (in ntel), and the loss function value (in f).\nreturn (structure (\n      list (\n        transform = v,\n        rhat = corList (v),\n        objectscores = h$x,\n        scores = y,\n        quantifications = z,\n        dmeasures = d,\n        lambda = dsum / ncol (data),\n        weights = a,\n        loadings = o,\n        ntel = h$ntel,\n        f = h$f\n      ),\n      class = \"homals\"\n    ))\nNote that in MCA we have \\(H_jA_j=G_jY_j\\). In previous Gifi publications the \\(Y_j\\) are called category quantifications. Our current homals() does not output the categaory quantifications directly, only the block scores \\(G_jY_j\\). If the \\(G_j\\) are binary indicators, the \\(Y_j\\) are just the distinct rows of \\(G_jY_j\\). There is also some indeterminacy in the representation \\(H_jA_j\\), which we resolve, at least partially, by using the QR decomposition \\(H_j=Q_jR_j\\) to replace \\(H_j\\) by \\(Q_j\\), and use \\(H_jA_j=Q_j(R_jA_j)\\). One small problem with this is that we may have \\(r_j\\df\\mathbf{rank}(H_j)&lt;r\\), in which case there are only \\(r_j\\) copies in \\(Q_j\\). This happens, for example, in the common case in which variable \\(j\\) is binary and takes only two values.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Correspondence Analysis and homals()</span>"
    ]
  },
  {
    "objectID": "mca.html#equations",
    "href": "mca.html#equations",
    "title": "4  Multiple Correspondence Analysis and homals()",
    "section": "4.2 Equations",
    "text": "4.2 Equations",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Correspondence Analysis and homals()</span>"
    ]
  },
  {
    "objectID": "mca.html#examples",
    "href": "mca.html#examples",
    "title": "4  Multiple Correspondence Analysis and homals()",
    "section": "4.3 Examples",
    "text": "4.3 Examples\n\n4.3.1 Hartigan’s Hardware\nOur first example are semi-serious data from Hartigan (1975) (p. 228), also analyzed in Gifi (1990) (p. 128-135). A number of screws, tacks, nails, and bolts are classified by six variables. The data are\n\n\n       thread head indentation bottom length brass\ntack        N    F           N      S      1     N\nnail1       N    F           N      S      4     N\nnail2       N    F           N      S      2     N\nnail3       N    F           N      F      2     N\nnail4       N    F           N      S      2     N\nnail5       N    F           N      S      2     N\nnail6       N    C           N      S      5     N\nnail7       N    C           N      S      3     N\nnail8       N    C           N      S      3     N\nscrew1      Y    O           T      S      5     N\nscrew2      Y    R           L      S      4     N\nscrew3      Y    Y           L      S      4     N\nscrew4      Y    R           L      S      2     N\nscrew5      Y    Y           L      S      2     N\nbolt1       Y    R           L      F      4     N\nbolt2       Y    O           L      F      1     N\nbolt3       Y    Y           L      F      1     N\nbolt4       Y    Y           L      F      1     N\nbolt5       Y    Y           L      F      1     N\nbolt6       Y    Y           L      F      1     N\ntack1       N    F           N      S      1     Y\ntack2       N    F           N      S      1     Y\nnailb       N    F           N      S      1     Y\nscrewb      Y    O           L      S      1     Y\n\n\nWe can do a simple MCA, using all the default values.\n\nh &lt;- homals (makeNumeric(hartigan))\n\nAfter 54 iterations we find a solution with loss 0.5157273. The object scores are plotted in figure\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe star plots, produced by the utility starPlotter() are in figure\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe discriminations matrices \\(\\Delta_j\\) are\n\n\n     [,1]  [,2] \n[1,]  0.93  0.16\n[2,]  0.16  0.03\n     [,1]  [,2] \n[1,]  0.96  0.04\n[2,]  0.04  0.64\n     [,1]  [,2] \n[1,]  0.94  0.07\n[2,]  0.07  0.66\n     [,1]  [,2] \n[1,]  0.39 -0.13\n[2,] -0.13  0.04\n     [,1]  [,2] \n[1,]  0.29 -0.19\n[2,] -0.19  0.82\n     [,1]  [,2] \n[1,]  0.07  0.05\n[2,]  0.05  0.03\n\n\nand their average \\(\\Lambda\\) is\n\n\n     [,1]  [,2] \n[1,]  0.60  0.00\n[2,]  0.00  0.37\n\n\nNote that the loss was 0.5157273, which is one minus the average of the trace of \\(\\Lambda\\). The induced correlations are\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] \n [1,]  1.00  1.00  0.01  0.98 -0.20  0.46  0.03  0.41 -0.22\n [2,]  1.00  1.00  0.00  0.98 -0.21  0.46  0.03  0.41 -0.22\n [3,]  0.01  0.00  1.00  0.08  0.38  0.18 -0.59  0.40 -0.02\n [4,]  0.98  0.98  0.08  1.00 -0.00  0.50 -0.10  0.43 -0.21\n [5,] -0.20 -0.21  0.38 -0.00  1.00  0.14 -0.66  0.05  0.09\n [6,]  0.46  0.46  0.18  0.50  0.14  1.00 -0.17  0.28 -0.29\n [7,]  0.03  0.03 -0.59 -0.10 -0.66 -0.17  1.00 -0.00 -0.10\n [8,]  0.41  0.41  0.40  0.43  0.05  0.28 -0.00  1.00  0.23\n [9,] -0.22 -0.22 -0.02 -0.21  0.09 -0.29 -0.10  0.23  1.00\n\n\nOf the six variables, three are binary. Thus they only have a single transformed variable associated with them, which is just the standardization to mean zero and sum of squares one. The total number of transformed variables is consequently 9. The eigenvalues of the induced correlation matrix (divided by the number of variables, not the number of transformed variables) are\n\n\n[1]  0.60  0.37  0.21  0.13  0.10  0.07  0.02  0.00  0.00\n\n\nNote that the two dominant eigenvalues are again equal to the diagonal elements of \\(\\Lambda\\).\n###GALO\nThe second example is somewhat more realistic. In the GALO dataset (Peschar (1975)) data on 1290 school children in the sixth grade of an elementary school in 1959 in the city of Groningen (Netherlands) were collected. The variables are gender, IQ (categorized into 9 ordered categories), advice (teacher categorized the children into 7 possible forms of secondary education, i.e., Agr = agricultural; Ext = extended primary education; Gen = general; Grls = secondary school for girls; Man = manual, including housekeeping; None = no further education; Uni = pre- University), SES (parent’s profession in 6 categories) and school (37 different schools). The data have been analyzed previously in many Gifi publications, for example in De Leeuw and Mair (2009). For our MCA we only make the first four variables, school is treated as passive\nWe use this example to illustrate some of the constraints on transformations. Two copies are used for all variables (although gender effectively only has one, of course). IQ is treated as ordinal, using a piecewise linear spline with knots at the nine data points.\n\ngalo_knots &lt;- knotsD(galo)\ngalo_degrees &lt;- c(-1,1,-1,-1,-1)\ngalo_ordinal &lt;- c(FALSE, TRUE, FALSE, FALSE,FALSE)\ngalo_active &lt;-c (TRUE, TRUE, TRUE, TRUE, FALSE)\n\n\nh &lt;- homals (galo, knots = galo_knots, degrees = galo_degrees, ordinal = galo_ordinal, active = galo_active)\n\nWe first give transformations for the active variables (and their copies) in figure . We skip gender, because transformation plots for binary variables are not very informative. We give two transformation plots for IQ, first using \\(H\\) and then using \\(HA\\). This illustrates the point made earlier, that transformation plots of block scores for ordinal variables with copies need not be monotone. It also illustrates that additional copies of an ordinal variable are not scaled to be monotone. Note that the plots for advice and SES are made with the utility stepPlotter(). Because the degree of the splines for those variables is zero, these transformation plots show step functions, with the steps at the knots, which are represented by vertical lines.\n\n\n\n\n\n\n\n\n\n\n\n\nThe four star plots for the active variables, together with the four category quantification plots, are in figure . Note that homals() does not compute category quantifications, we have to compute them from the homals() output. Also note that for gender, advice and SES the object scores are connected to the category centroids of the variables. For IQ object scores are connected to points on the line connecting adjacent category quantifications. See De Leeuw and Rijckevorsel (1988) for category plots using forms of fuzzy coding (of which B-splines are an example).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor this analysis we need 52 iterations to obtain loss 0.54251. The average discrimination matrix over the four active variables is\n\n\n     [,1]  [,2] \n[1,]  0.54  0.00\n[2,]  0.00  0.38\n\n\nwhile the eigenvalues of the induced correlation matrix of the active variables and their copies, divided by four, are\n\n\n[1]  0.54  0.38  0.26  0.21  0.18  0.13  0.05\n\n\nThe category quantifications for the passive variable indicating the 37 schools are in figure\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we look at the scale of the plot we see all schools are pretty close to the origin. The discrimination matrices are consequently also small. In 1959 schools were pretty much the same.\n\n\n     [,1]    [,2]   \n[1,]  0.0011 -0.0014\n[2,] -0.0014  0.0022\n\n\n\n\n4.3.2 Thirteen Personality Scales\nOur next example is a small data block from the psych package (Revelle 2015) of five scales from the Eysenck Personality Inventory, five from a Big Five inventory, a Beck Depression Inventory, and State and Trait Anxiety measures.\n\nepi&lt;- read.csv(\"data/epi.bfi.csv\")\nepi_knots &lt;- knotsQ(epi)\nepi_degrees &lt;- rep (0, 13)\nepi_ordinal &lt;- rep (FALSE, 13)\n\nWe perform a two-dimensional MCA, using degree zero and inner knots at the three quartiles for all 13 variables.\n\nh &lt;- homals(epi, knots = epi_knots, degrees = epi_degrees, ordinal = epi_ordinal)\n\nWe have convergence in 271 iterations to loss 0.7472906. The object scores are in figure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe thirteen star plots are in figure\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow change the degree to two for all variables, i.e. fit piecewise quadratic polynomials which are differentiable at the knots. We still have two copies for each variable, and these two copies define the blocks.\n\nepi_degrees &lt;- rep (2, 13)\nh &lt;- homals (epi, knots = epi_knots, degrees = epi_degrees, ordinal = epi_ordinal)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDe Leeuw, J., and P. Mair. 2009. “Homogeneity Analysis in R: the Package homals.” Journal of Statistical Software 31 (4): 1–21. https://www.jstatsoft.org/v31/i04/.\n\n\nDe Leeuw, J., and J. L. A. Van Rijckevorsel. 1988. “Beyond Homogeneity Analysis.” In Component and Correspondence Analysis, edited by J. L. A. Van Rijckevorsel and J. De Leeuw, 55–80. Chichester, England: Wiley.\n\n\nGifi, A. 1990. Nonlinear Multivariate Analysis. New York, N.Y.: Wiley.\n\n\nHartigan, J. 1975. Clustering Algorithms. Wiley.\n\n\nPeschar, J. L. 1975. School, Milieu, Beroep. Groningen, The Netherlands: Tjeek Willink.\n\n\nRevelle, William. 2015. Psych: Procedures for Psychological, Psychometric, and Personality Research. Evanston, Illinois: Northwestern University.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multiple Correspondence Analysis and homals()</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bijleveld, C. C. J. H., and J. De Leeuw. 1991. “Fitting Longitudinal Reduced-Rank Regression Models by\nAlternating Least Squares.” Psychometrika 56 (3):\n433–47.\n\n\nBock, R. D. 1960. “Methods and Applications\nof Optimal Scaling.” Psychometric Laboratory Report 25.\nChapell Hill, N.C.: L.L. Thurstone Psychometric Laboratory, University\nof North Carolina.\n\n\nBreiman, L., and J. H. Friedman. 1985. “Estimating Optimal Transformations for Multiple\nRegression and Correlation.” Journal of the American\nStatistical Association 80: 580–619.\n\n\nBurg, E. Van der, J. De Leeuw, and R. Verdegaal. 1988.\n“Homogeneity Analysis with K Sets of Variables: An\nAlternating Least Squares Approach with Optimal Scaling\nFeatures.” Psychometrika 53: 177–97.\n\n\nCarroll, J. D. 1968. “A Generalization of\nCanonical Correlation Analysis to Three or More Sets of\nVariables.” In Proceedings of the 76th Annual\nConvention of the American Psychological Association, 227–28.\nWashington, D.C.: American Psychological Association.\n\n\nCoolen, H., and J. De Leeuw. 1987. “Least\nSquares Path Analysis with Optimal Scaling.” Research\nReport RR-87-03. Leiden, The Netherlands: Department of Data Theory\nFSW/RUL.\n\n\nCordier, B. 1965. “L’Analyse Factorielle des\nCorrespondances.” {Th\\`ese de Troisieme Cycle}, Université\nde Rennes.\n\n\nDe Leeuw, J. 1968a. “Canonical Discriminant Analysis of Relational\nData.” Research Note 007-68. Department of Data Theory FSW/RUL.\nhttps://jansweb.netlify.app/publication/deleeuw-r-68-e/deleeuw-r-68-e.pdf.\n\n\n———. 1968b. “Nonmetric Discriminant Analysis.” Research\nNote 06-68. Department of Data Theory, University of Leiden.\n\n\n———. 1974. Canonical Analysis of Categorical Data. Leiden, The\nNetherlands: Psychological Institute, Leiden University.\n\n\n———. 1975. “A Normalized Cone Regression\nApproach to Alternating Least Squares Algorithms.”\nDepartment of Data Theory FSW/RUL. https://jansweb.netlify.app/publication/deleeuw-u-75-a/deleeuw-u-75-a.pdf.\n\n\n———. 1983. “On the Prehistory of Correspondence Analysis.”\nStatistica Neerlandica 37: 161–64.\n\n\n———. 1984. “The Gifi System of Nonlinear Multivariate\nAnalysis.” In Data Analysis and Informatics, edited by\nE. Diday et al. Vol. III. Amsterdam: North Holland Publishing Company.\n\n\n———. 1988. “Multivariate Analysis with\nOptimal Scaling.” In Proceedings of the International\nConference on Advances in Multivariate Statistical Analysis, edited\nby S. Das Gupta and J. K. Ghosh, 127–60. Calcutta, India: Indian\nStatistical Institute.\n\n\n———. 1994. “Block Relaxation Algorithms in\nStatistics.” In Information Systems and Data\nAnalysis, edited by H. H. Bock, W. Lenski, and M. M. Richter,\n308–24. Berlin: Springer Verlag. https://jansweb.netlify.app/publication/deleeuw-c-94-c/deleeuw-c-94-c.pdf.\n\n\n———. 2004. “Least Squares Optimal Scaling of Partially Observed\nLinear Systems.” In Recent Developments in Structural\nEquation Models, edited by K. van Montfort, J. Oud, and A. Satorra.\nDordrecht, Netherlands: Kluwer Academic Publishers.\n\n\n———. 2009. “Regression, Discriminant\nAnalysis, and Canonical Analysis with homals.” Preprint\nSeries 562. Los Angeles, CA: UCLA Department of Statistics.\n\n\n———. 2015a. “Aspects of Correlation\nMatrices.” https://jansweb.netlify.app/publication/deleeuw-e-15-b/deleeuw-e-15-b.pdf.\n\n\n———. 2015b. “Exceedingly Simple b-Spline Code.”\n\n\n———. 2015c. “Regression with Linear Inequality Restrictions on\nPredicted Values.”\n\n\nDe Leeuw, J., and P. Mair. 2009. “Homogeneity\nAnalysis in R: the Package homals.”\nJournal of Statistical Software 31 (4): 1–21. https://www.jstatsoft.org/v31/i04/.\n\n\nDe Leeuw, J., and J. L. A. Van Rijckevorsel. 1980.\n“HOMALS and PRINCALS: Some\nGeneralizations of Principal Components Analysis.” In Data\nAnalysis and Informatics. Amsterdam: North Holland Publishing\nCompany.\n\n\n———. 1988. “Beyond Homogeneity Analysis.” In Component\nand Correspondence Analysis, edited by J. L. A. Van Rijckevorsel\nand J. De Leeuw, 55–80. Chichester, England: Wiley.\n\n\nDe Leeuw, J., J. L. A. Van Rijckevorsel, and H. Van der Wouden. 1981.\n“Nonlinear Principal Component Analysis Using b-Splines.”\nMethods of Operations Research 43: 379–94.\n\n\nGibson, W. A. 1962. “On the Least Squares\nOrthogonalization of an Oblique Transformation.”\nPsychometrika 27: 193–95.\n\n\nGifi, A. 1980. Niet-Lineaire Multivariate Analyse [Nonlinear\nMultivariate Analysis]. Leiden, The Netherlands: Department of Data\nTheory FSW/RUL.\n\n\n———. 1981. Nonlinear Multivariate Analysis. Leiden, The\nNetherlands: Department of Data Theory FSW/RUL.\n\n\n———. 1990. Nonlinear Multivariate Analysis. New York, N.Y.:\nWiley.\n\n\nGower, J. C. 1990. “Fisher’s Optimal Scores\nand Multiple Correspondence Analysis.” Biometrics\n46: 947–61.\n\n\nGower, J. C., and D. J. Hand. 1996. Biplots.\nMonographs on Statistics and Applied Probability 54. Chapman &\nHall.\n\n\nGower, J. C., N. J. Le Roux, and S. Gardner-Lubbe. 2015.\n“Biplots: Quantitative Data.” WIREs\nComputational Statistics 7: 42–62.\n\n\n———. 2016. “Biplots: Qualitative Data.”\nWIREs Computational Statistics 8: 82–111.\n\n\nGuttman, L. 1941. “The Quantification of a\nClass of Attributes: A Theory and Method of Scale\nConstruction.” In The Prediction of Personal\nAdjustment, edited by P. Horst, 321–48. New York: Social Science\nResearch Council.\n\n\nHartigan, J. 1975. Clustering Algorithms. Wiley.\n\n\nHill, M. O. 1974. “Correspondence Analysis: a\nNeglected Multivariate Method.” Applied\nStatistics 23: 340–54.\n\n\n———. 1990. “Review of A. Gifi, Multivariate\nAnalysis.” Journal of Ecology 78 (4): 1148–49.\n\n\nHolland, P. W. 1979. “The Tyranny of\nContinuous Models in a World of Discrete Data.”\nIHS-Journal 3: 29–42.\n\n\nIBM. 2015. IBM SPSS Categories 23. IBM Corporation.\n\n\nKoyak, R. 1987. “On Measuring Internal\nDependence in a Set of Random Variables.” Annals of\nStatistics 15: 1215–28.\n\n\nKruskal, J. B. 1964a. “Multidimensional\nScaling by Optimizing Goodness of Fit to a Nonmetric\nHypothesis.” Psychometrika 29: 1–27.\n\n\n———. 1964b. “Nonmetric Multidimensional\nScaling: a Numerical Method.” Psychometrika 29:\n115–29.\n\n\n———. 1965. “Analysis of Factorial Experiments\nby Estimating Monotone Transformations of the Data.”\nJournal of the Royal Statistical Society B27: 251–63.\n\n\nKruskal, J. B., and R. N. Shepard. 1974. “A\nNonmetric Variety of Linear Factor Analysis.”\nPsychometrika 39: 123–57.\n\n\nLingoes, J. C. 1973. The Guttman-Lingoes Nonmetric Program\nSeries. Mathesis Press.\n\n\nMair, P., and J. De Leeuw. 2010. “A General Framework for\nMultivariate Analysis with Optimal Scaling: The r Package\nAspect.” Journal of Statistical Software 32 (9): 1–23.\n\n\nMarvell, A. 1653. “The Character of Holland.”\n\n\nMeulman, J. J. 1982. Homogeneity Analysis of\nIncomplete Data. Leiden, The Netherlands: DSWO\nPress.\n\n\nMeulman, J. J., and W. J. Heiser. 2012. IBM SPSS Categories 21.\nIBM Corporation.\n\n\nMichailidis, G., and J. De Leeuw. 1998. “The Gifi System for\nDescriptive Multivariate Analysis.” Statistical Science\n13: 307–36.\n\n\nPeschar, J. L. 1975. School, Milieu, Beroep.\nGroningen, The Netherlands: Tjeek Willink.\n\n\nRevelle, William. 2015. Psych: Procedures for Psychological,\nPsychometric, and Personality Research. Evanston, Illinois:\nNorthwestern University.\n\n\nRoskam, E. E. 1968. “Metric Analysis of\nOrdinal Data in Psychology.” PhD thesis, University of\nLeiden.\n\n\nSPSS. 1989. SPSS Categories. SPSS Inc.\n\n\nTakane, Y. 1992. “Review of Albert Gifi,\nNonlinear Multivariate Analysis.” Journal of the\nAmerican Statistical Association 87: 587–88.\n\n\nTanaka, Y. 1979. “Review of the Methods of\nQuantification.” Environmental Health\nPerspectives 32: 113–23.\n\n\nVan der Heijden, P. G. M., and S. Van Buuren. 1916. “Looking Back\nat the Gifi System of Nonlinear Multivariate Analysis.”\nJournal of Statistical Software 73 (4).\n\n\nVan Rijckevorsel, J. L. A., and J. De Leeuw, eds. 1988. Component\nand Correspondence Analysis. Wiley.\n\n\nWinsberg, S., and J. O. Ramsay. 1980. “Monotone Transformations to Additivity.”\nBiometrika 67: 669–74.\n\n\n———. 1983. “Monotone Spline Transformations\nfor Dimension Reduction.” Psychometrika 48:\n575–95.\n\n\nXie, Y. 2015. Dynamic Documents with R and\nknitr. Second Edition. CRC Press.\n\n\n———. 2016. Bookdown: Authoring Books with r Markdown.\n\n\nYoung, F. W. 1981. “Quantitative Analysis of\nQualitative Data.” Psychometrika 46: 357–88.\n\n\nYoung, F. W., J. De Leeuw, and Y. Takane. 1980. “Quantifying\nQualitative Data.” In Similarity and Choice. Papers in Honor\nof Clyde Coombs, edited by E. D. Lantermann and H. Feger. Bern:\nHans Huber.",
    "crumbs": [
      "References"
    ]
  }
]